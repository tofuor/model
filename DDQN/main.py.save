import gym
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Variable
import numpy as np
import random
from collections import deque

# 超參數
EPISODES = 500
EPS_START = 0.9  # 隨機選擇行動的概率
EPS_END = 0.05
EPS_DECAY = 200  # epsilon 衰減的速度
GAMMA = 0.8  # 折扣因子
LR = 0.001  # 學習率
MEMORY_SIZE = 10000  # 經驗回放的記憶體大小
BATCH_SIZE = 128  # 訓練批次的大小

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class DQN(nn.Module):
    def __init__(self):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(4, 24)
        self.fc2 = nn.Linear(24, 48)
        self.fc3 = nn.Linear(48, 2)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
        return x

class DoubleDQNAgent:
    def __init__(self):
        self.model = DQN().to()
        self.target_model = DQdeviceN()
        self.update_target_model()
        self.memory = deque(maxlen=MEMORY_SIZE)
        self.optimizer = optim.Adam(self.model.parameters(), LR)
        self.steps_done = 0

    def update_target_model(self):
        self.target_model.load_state_dict(self.model.state_dict())

    def memorize(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def act(self, state):
        eps_threshold = EPS_END + (EPS_START - EPS_END) * \
            np.exp(-1. * self.steps_done / EPS_DECAY)
        self.steps_done += 1
        if np.random.rand() <= eps_threshold:
            return random.randrange(2)
        else:
            return torch.argmax(self.model(Variable(state))).item()

    def replay(self):
        if len(self.memory) < BATCH_SIZE:
            return
        batch = random.sample(self.memory, BATCH_SIZE)
        for state, action, reward, next_state, done in batch:
            state = Variable(state)
            next_state = Variable(next_state)
            reward = torch.from_numpy(np.array([reward], dtype=np.float32)).unsqueeze(0)
            done = torch.from_numpy(np.array([done], dtype=np.int32)).unsqueeze(0)

            q_values = self.model(state)
            next_q_values = self.model(next_state)
            next_q_state_values = self.target_model(next_state)

            q_value = q_values.gather(1, torch.LongTensor([[action]]))
            next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1))
            expected_q_value = reward + GAMMA * next_q_value * (1 - done)

            loss = nn.MSELoss()(q_value, expected_q_value.detach())

            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

env = gym.make('CartPole-v0')
agent = DoubleDQNAgent()

for e in range(EPISODES):
    state = env.reset()
    # print(type(state))  # 印出state的類型
    # print(state)  # 印出state的值
    state = torch.tensor(state[0], dtype=torch.float32).unsqueeze(0)
    # state = torch.from_numpy(state).float().unsqueeze(0)
    for time_t in range(500):
        action = agent.act(state)
        # step_result = env.step(action)
        # print(step_result)

        next_state, reward, done, _, _ = env.step(action)
        reward = reward if not done else -10
        next_state = torch.from_numpy(next_state).float().unsqueeze(0)
        agent.memorize(state, action, reward, next_state, done)
        state = next_state
        agent.replay()
        if done:
            print(f"遊戲結束於 {time_t+1} 步數，於第 {e+1} 輪")
            break
    if e % 10 == 0:
        agent.update_target_model()
